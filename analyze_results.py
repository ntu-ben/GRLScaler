#!/usr/bin/env python3
"""
å¯¦é©—çµæžœåˆ†æžå·¥å…·
================

åˆ†æžä¸‰ç¨®æ–¹æ³•çš„è² è¼‰æ¸¬è©¦çµæžœï¼š
- GNNRL (åœ–ç¥žç¶“ç¶²è·¯å¼·åŒ–å­¸ç¿’)
- Gym-HPA (åŸºç¤Žå¼·åŒ–å­¸ç¿’)  
- K8s-HPA (åŽŸç”Ÿ HPA åŸºæº–)
"""

import pandas as pd
import json
from pathlib import Path
import sys

def analyze_locust_results(stats_file):
    """åˆ†æž Locust æ¸¬è©¦çµ±è¨ˆæª”æ¡ˆ"""
    if not Path(stats_file).exists():
        return None
    
    df = pd.read_csv(stats_file)
    aggregated_row = df[df['Type'] == ''].iloc[0] if any(df['Type'] == '') else df.iloc[-1]
    
    return {
        'total_requests': int(aggregated_row['Request Count']),
        'failure_rate': float(aggregated_row['Failure Count']) / float(aggregated_row['Request Count']) * 100,
        'avg_rps': float(aggregated_row['Requests/s']),
        'avg_response_time': float(aggregated_row['Average Response Time']),
        'median_response_time': float(aggregated_row['Median Response Time']),
        'p95_response_time': float(aggregated_row['95%']),
        'p99_response_time': float(aggregated_row['99%'])
    }

def analyze_kiali_graph(kiali_file):
    """åˆ†æž Kiali æœå‹™åœ–æª”æ¡ˆ"""
    if not Path(kiali_file).exists():
        return None
        
    with open(kiali_file) as f:
        data = json.load(f)
    
    nodes = data.get('elements', {}).get('nodes', [])
    edges = data.get('elements', {}).get('edges', [])
    
    # çµ±è¨ˆæœå‹™
    services = []
    total_traffic = 0
    
    for node in nodes:
        workload = node['data'].get('workload', 'unknown')
        services.append(workload)
        
        # çµ±è¨ˆæµé‡
        traffic = node['data'].get('traffic', [])
        for t in traffic:
            rates = t.get('rates', {})
            for rate_type, rate_value in rates.items():
                if rate_value and rate_value != '0':
                    try:
                        total_traffic += float(rate_value)
                    except:
                        pass
    
    return {
        'service_count': len(services),
        'edge_count': len(edges),
        'services': services,
        'total_traffic_rate': total_traffic
    }

def find_experiment_results(experiment_type):
    """æ‰¾åˆ°å¯¦é©—çµæžœç›®éŒ„ï¼Œåˆ†åˆ¥è™•ç†è¨“ç·´å’Œæ¸¬è©¦æ•¸æ“š"""
    logs_dir = Path(__file__).parent / 'logs' / experiment_type
    if not logs_dir.exists():
        return [], "unknown"
    
    # å„ªå…ˆå°‹æ‰¾æ¸¬è©¦ç›®éŒ„
    test_dirs = [d for d in logs_dir.iterdir() if d.is_dir() and 'test' in d.name]
    if test_dirs:
        latest_dir = max(test_dirs, key=lambda d: d.stat().st_mtime)
        data_type = "test"
    else:
        # å¦‚æžœæ²’æœ‰testç›®éŒ„ï¼Œä½¿ç”¨trainç›®éŒ„
        train_dirs = [d for d in logs_dir.iterdir() if d.is_dir() and ('train' in d.name or 'cpu' in d.name)]
        if train_dirs:
            latest_dir = max(train_dirs, key=lambda d: d.stat().st_mtime)
            data_type = "train" if 'train' in latest_dir.name else "test"
        else:
            return [], "unknown"
    
    # å°æ–¼k8s-hpaï¼Œéœ€è¦é€²ä¸€æ­¥æŸ¥æ‰¾cpué…ç½®ç›®éŒ„
    if experiment_type == 'k8s-hpa':
        cpu_dirs = [d for d in latest_dir.iterdir() if d.is_dir() and 'cpu-' in d.name]
        if cpu_dirs:
            all_scenarios = []
            for cpu_dir in cpu_dirs:
                all_scenarios.extend(list(cpu_dir.glob('*/')))
            return all_scenarios, data_type
    
    return list(latest_dir.glob('*/')), data_type

def generate_comparison_report():
    """ç”Ÿæˆä¸‰ç¨®æ–¹æ³•çš„æ¯”è¼ƒå ±å‘Š"""
    print("ðŸ” å¯¦é©—çµæžœåˆ†æžå ±å‘Š")
    print("=" * 60)
    
    experiments = {
        'GNNRL': 'gnnrl',
        'Gym-HPA': 'gym-hpa', 
        'K8s-HPA': 'k8s-hpa'
    }
    
    all_results = {}
    
    for exp_name, exp_type in experiments.items():
        scenario_dirs, data_type = find_experiment_results(exp_type)
        
        print(f"\nðŸ“Š {exp_name} çµæžœåˆ†æž")
        if data_type == "train":
            print(f"âš ï¸  ä½¿ç”¨è¨“ç·´éšŽæ®µæ•¸æ“š (æœªæ‰¾åˆ°æ¸¬è©¦æ•¸æ“š)")
        elif data_type == "test":
            print(f"âœ… ä½¿ç”¨æ¸¬è©¦éšŽæ®µæ•¸æ“š")
        print("-" * 40)
        
        if not scenario_dirs:
            print(f"âŒ æœªæ‰¾åˆ° {exp_name} çš„çµæžœæ•¸æ“š")
            continue
        
        exp_results = []
        
        for scenario_dir in scenario_dirs:
            stats_file = scenario_dir / f"{scenario_dir.name.split('_')[0]}_stats.csv"
            
            if stats_file.exists():
                result = analyze_locust_results(stats_file)
                if result:
                    result['scenario'] = scenario_dir.name
                    exp_results.append(result)
                    
                    print(f"  ðŸ“ˆ {scenario_dir.name}:")
                    print(f"    è«‹æ±‚æ•¸: {result['total_requests']:,}")
                    print(f"    å¤±æ•—çŽ‡: {result['failure_rate']:.2f}%")
                    print(f"    å¹³å‡ RPS: {result['avg_rps']:.2f}")
                    print(f"    å¹³å‡éŸ¿æ‡‰æ™‚é–“: {result['avg_response_time']:.2f} ms")
                    print(f"    95%ile: {result['p95_response_time']:.0f} ms")
        
        all_results[exp_name] = exp_results
        
        if exp_results:
            # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
            total_requests = sum(r['total_requests'] for r in exp_results)
            avg_response_time = sum(r['avg_response_time'] * r['total_requests'] for r in exp_results) / total_requests if total_requests > 0 else 0
            avg_p95 = sum(r['p95_response_time'] for r in exp_results) / len(exp_results)
            
            data_note = " (è¨“ç·´æ•¸æ“š)" if data_type == "train" else " (æ¸¬è©¦æ•¸æ“š)"
            print(f"  ðŸ“‹ {exp_name} ç¸½è¨ˆ{data_note}:")
            print(f"    å ´æ™¯æ•¸: {len(exp_results)}")
            print(f"    ç¸½è«‹æ±‚æ•¸: {total_requests:,}")
            print(f"    åŠ æ¬Šå¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response_time:.2f} ms")
            print(f"    å¹³å‡ 95%ile: {avg_p95:.0f} ms")
    
    # ç”Ÿæˆæ¯”è¼ƒè¡¨æ ¼
    if len(all_results) > 1:
        print(f"\nðŸ† æ–¹æ³•æ¯”è¼ƒæ‘˜è¦")
        print("-" * 50)
        
        comparison_data = []
        for exp_name, results in all_results.items():
            if results:
                total_requests = sum(r['total_requests'] for r in results)
                avg_response_time = sum(r['avg_response_time'] * r['total_requests'] for r in results) / total_requests if total_requests > 0 else 0
                avg_p95 = sum(r['p95_response_time'] for r in results) / len(results)
                avg_rps = sum(r['avg_rps'] for r in results) / len(results)
                
                comparison_data.append({
                    'Method': exp_name,
                    'Scenarios': len(results),
                    'Total Requests': total_requests,
                    'Avg Response Time (ms)': round(avg_response_time, 2),
                    'Avg P95 (ms)': round(avg_p95, 0),
                    'Avg RPS': round(avg_rps, 2)
                })
        
        if comparison_data:
            df_comparison = pd.DataFrame(comparison_data)
            print(df_comparison.to_string(index=False))
            
            # ä¿å­˜æ¯”è¼ƒçµæžœ
            comparison_file = Path(__file__).parent / 'logs' / 'experiment_comparison.csv'
            df_comparison.to_csv(comparison_file, index=False)
            print(f"\nðŸ’¾ æ¯”è¼ƒçµæžœå·²ä¿å­˜åˆ°: {comparison_file}")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--experiment":
        if len(sys.argv) > 2:
            exp_type = sys.argv[2]
            scenario_dirs = find_experiment_results(exp_type)
            print(f"Found {len(scenario_dirs)} scenarios for {exp_type}")
        else:
            print("Usage: python analyze_results.py --experiment <type>")
    else:
        generate_comparison_report()