# GNNRL å®Œæ•´é‹ä½œéç¨‹æŒ‡å—

## ç³»çµ±æ¶æ§‹æ¦‚è¿°

GNNRL (Graph Neural Network Reinforcement Learning) æ˜¯ä¸€å€‹åŸºæ–¼åœ–ç¥ç¶“ç¶²è·¯çš„å¼·åŒ–å­¸ç¿’ç³»çµ±ï¼Œå°ˆé–€ç”¨æ–¼Kubernetesç’°å¢ƒä¸­çš„å‹•æ…‹è³‡æºèª¿åº¦ã€‚ç³»çµ±æ”¯æ´çœŸæ­£çš„å‹•æ…‹åœ–çµæ§‹ï¼Œèƒ½å¤ è™•ç†ç¯€é»å¢æ¸›çš„æƒ…æ³ã€‚

## ğŸ—ï¸ æ ¸å¿ƒçµ„ä»¶æ¶æ§‹

### 1. å‹•æ…‹åœ–ç®¡ç†å±¤ (`DynamicGraphSpace`)
**æª”æ¡ˆ**: `gnnrl/core/envs/dynamic_graph_space.py`

```python
class DynamicGraphSpace:
    """å‹•æ…‹åœ–ç©ºé–“ç®¡ç†å™¨ï¼Œæ”¯æ´å¯è®Šç¯€é»å’Œé‚Šæ•¸é‡"""
    
    def __init__(self, config: DynamicGraphConfig):
        self.config = config
        self.node_mapping: Dict[str, int] = {}
        self.edge_mapping: Dict[tuple, int] = {}
        
    def update_node_mapping(self, service_names: list) -> Dict[str, int]:
        """æ›´æ–°ç¯€é»æ˜ å°„ï¼Œè™•ç†æœå‹™å¢æ¸›"""
        # å¯¦ç¾ç¯€é»IDçš„å‹•æ…‹åˆ†é…
        
    def pad_node_features(self, features: np.ndarray, num_nodes: int) -> Tuple[np.ndarray, np.ndarray]:
        """å¡«å……ç¯€é»ç‰¹å¾µåˆ°æœ€å¤§ç¶­åº¦ï¼Œè¿”å›ç‰¹å¾µå’Œé®ç½©"""
        
    def pad_edge_features(self, edges: np.ndarray, num_edges: int) -> Tuple[np.ndarray, np.ndarray]:
        """å¡«å……é‚Šç‰¹å¾µåˆ°æœ€å¤§ç¶­åº¦ï¼Œè¿”å›ç‰¹å¾µå’Œé®ç½©"""
```

**é—œéµç‰¹æ€§**:
- æ”¯æ´æœ€å¤§20å€‹ç¯€é»çš„å‹•æ…‹æ“´å±•
- è‡ªå‹•è™•ç†ç¯€é»æ˜ å°„è®ŠåŒ–
- æä¾›paddingå’Œmaskingæ©Ÿåˆ¶
- é‚Šç‰¹å¾µåŒ…å«7å€‹ç¶­åº¦ï¼š[src, dst, active, qps, p95, err_rate, mtls_percent]

### 2. ç’°å¢ƒå±¤ (`OnlineBoutique`)
**æª”æ¡ˆ**: `gnnrl/core/envs/online_boutique.py`

```python
class OnlineBoutique(gym.Env):
    """OnlineBoutique Kubernetesç’°å¢ƒ"""
    
    def __init__(self, k8s=False, goal_reward="cost", use_graph=False):
        # åˆå§‹åŒ–å‹•æ…‹åœ–ç©ºé–“
        if self.use_graph:
            config = DynamicGraphConfig(max_nodes=20, max_edges=400, ...)
            self.dynamic_graph = DynamicGraphSpace(config)
    
    def _fetch_service_graph(self):
        """å¾Kialiç²å–æœå‹™åœ–æ•¸æ“š"""
        # æ”¯æ´å‹•æ…‹ç¯€é»æ•¸é‡
        active_services = [name for name in nodes if name in DEPLOYMENTS]
        node_mapping = self.dynamic_graph.update_node_mapping(active_services)
        return padded_edges, edge_mask, len(active_services)
    
    def get_state(self):
        """ç²å–ç’°å¢ƒç‹€æ…‹"""
        if self.use_graph:
            return {
                'svc_df': padded_nodes,      # ç¯€é»ç‰¹å¾µ (max_nodes, 6)
                'edge_df': padded_edges,     # é‚Šç‰¹å¾µ (max_edges, 7)
                'global_feats': padded_global, # å…¨å±€ç‰¹å¾µ (4,)
                'node_mask': node_mask,      # ç¯€é»é®ç½©
                'edge_mask': edge_mask,      # é‚Šé®ç½©
                'num_nodes': num_active_nodes,
                'invalid_action_mask': mask,
            }
```

**é—œéµç‰¹æ€§**:
- æ”¯æ´11å€‹å›ºå®šæœå‹™ + å‹•æ…‹æ“´å±•æœå‹™
- å‹•ä½œç©ºé–“ï¼š15å€‹å‹•ä½œ Ã— 11å€‹æœå‹™ = 165ç¶­
- çœŸå¯¦Kialiæ•¸æ“šç²å–å’Œè™•ç†
- å‹•æ…‹è§€å¯Ÿç©ºé–“é©é…

### 3. æ™‚åºåœ–ç·¨ç¢¼å™¨ (`DynamicTGNEncoder`)
**æª”æ¡ˆ**: `gnnrl/encoders/tgn_encoder.py`

```python
class DynamicTGNEncoder(nn.Module):
    """æ”¯æ´å‹•æ…‹ç¯€é»æ˜ å°„çš„TGNç·¨ç¢¼å™¨"""
    
    def __init__(self, max_nodes: int, in_dim: int, memory_dim: int = 32):
        self.memory = TGNMemory(num_nodes=max_nodes, ...)
        self.conv = TransformerConv(in_dim, memory_dim, heads=2)
        self.node_mapping: Dict[str, int] = {}
        
    def update_node_mapping(self, service_names: list) -> Dict[str, int]:
        """æ›´æ–°ç¯€é»æ˜ å°„ä¸¦é‡æ–°åˆ†é…è¨˜æ†¶é«”"""
        if old_mapping != new_mapping:
            self._remap_memory(old_mapping, new_mapping)
            
    def _remap_memory(self, old_mapping: Dict[str, int], new_mapping: Dict[str, int]):
        """é‡æ–°æ˜ å°„TGNè¨˜æ†¶é«”ç‹€æ…‹"""
        # ä¿å­˜ç¾æœ‰æœå‹™çš„è¨˜æ†¶é«”ç‹€æ…‹
        # é‡ç½®è¨˜æ†¶é«”ä¸¦æ¢å¾©ä¿å­˜çš„ç‹€æ…‹
        
    def forward(self, edge_data, node_features, edge_mask, node_mask):
        """å‹•æ…‹åœ–å‰å‘å‚³æ’­"""
        # æ›´æ–°æ™‚åºè¨˜æ†¶é«”
        # æ‡‰ç”¨Transformerå·ç©
        # è¿”å›ç¯€é»è¡¨ç¤º
```

**é—œéµç‰¹æ€§**:
- æ”¯æ´ç¯€é»æ˜ å°„è®ŠåŒ–æ™‚çš„è¨˜æ†¶é«”é‡æ–°åˆ†é…
- æ™‚åºè¨˜æ†¶é«”æ©Ÿåˆ¶ä¿æŒæœå‹™é–“çš„æ­·å²ä¾è³´
- Transformeræ³¨æ„åŠ›æ©Ÿåˆ¶æ•æ‰æœå‹™é–“é—œä¿‚

### 4. ç­–ç•¥ç¶²è·¯ (`PPO_GNN`)
**æª”æ¡ˆ**: `gnnrl/core/agents/ppo_gnn.py`

```python
class PPO_GNN:
    """åŸºæ–¼GNNçš„PPOä»£ç†"""
    
    def __init__(self, obs_space, action_space, use_tgn=False):
        if use_tgn:
            self.tgn_encoder = DynamicTGNEncoder(max_nodes=20, in_dim=6)
        self.gnn_encoder = GNNEncoder(node_dim=6, edge_dim=7)
        self.policy_head = PolicyHead(hidden_dim=128, action_dim=action_space.nvec)
        
    def forward(self, obs):
        """å‰å‘å‚³æ’­"""
        # 1. TGNç·¨ç¢¼æ™‚åºä¿¡æ¯
        if self.use_tgn:
            temporal_features = self.tgn_encoder(
                obs['edge_df'], obs['svc_df'], 
                obs['edge_mask'], obs['node_mask']
            )
        
        # 2. GNNç·¨ç¢¼åœ–çµæ§‹
        graph_features = self.gnn_encoder(
            obs['svc_df'], obs['edge_df'], obs['node_mask']
        )
        
        # 3. ç­–ç•¥å’Œåƒ¹å€¼é æ¸¬
        actions, values = self.policy_head(graph_features)
        return actions, values
```

## ğŸ”„ å®Œæ•´é‹ä½œæµç¨‹

### Phase 1: ç’°å¢ƒåˆå§‹åŒ–
```python
# 1. å‰µå»ºç’°å¢ƒå¯¦ä¾‹
env = OnlineBoutique(k8s=True, use_graph=True, goal_reward="latency")

# 2. åˆå§‹åŒ–å‹•æ…‹åœ–ç©ºé–“
config = DynamicGraphConfig(max_nodes=20, max_edges=400)
env.dynamic_graph = DynamicGraphSpace(config)

# 3. åˆå§‹åŒ–PPOä»£ç†
agent = PPO_GNN(env.observation_space, env.action_space, use_tgn=True)
```

### Phase 2: æ•¸æ“šç²å–èˆ‡é è™•ç†
```python
def step_data_flow():
    # 1. å¾Kialiç²å–æœå‹™åœ–
    nodes, edge_df = fetch_service_graph(namespace="onlineboutique")
    
    # 2. æ›´æ–°ç¯€é»æ˜ å°„
    active_services = [name for name in nodes if name in DEPLOYMENTS]
    node_mapping = env.dynamic_graph.update_node_mapping(active_services)
    
    # 3. å»ºæ§‹é‚Šç‰¹å¾µ
    edges = []
    for _, row in edge_df.iterrows():
        src_name, dst_name = nodes[row["src"]], nodes[row["dst"]]
        if src_name in node_mapping and dst_name in node_mapping:
            s, d = node_mapping[src_name], node_mapping[dst_name]
            edges.append([s, d, 1.0, row["qps"], row["p95"], row["err_rate"], row.get("mtls", 0)])
    
    # 4. å¡«å……åˆ°æœ€å¤§ç¶­åº¦
    padded_edges, edge_mask = env.dynamic_graph.pad_edge_features(edges, len(edges))
    
    # 5. ç²å–ç¯€é»ç‰¹å¾µ
    node_features = []
    for d in env.deploymentList:
        node_features.append([d.num_pods, d.desired_replicas, d.cpu_usage, 
                            d.mem_usage, d.received_traffic, d.transmit_traffic])
    
    padded_nodes, node_mask = env.dynamic_graph.pad_node_features(node_features, len(active_services))
```

### Phase 3: åœ–ç¥ç¶“ç¶²è·¯è™•ç†
```python
def gnn_processing_flow(obs):
    # 1. TGNæ™‚åºç·¨ç¢¼
    if agent.use_tgn:
        # æ›´æ–°ç¯€é»æ˜ å°„
        agent.tgn_encoder.update_node_mapping(active_services)
        
        # æ™‚åºç‰¹å¾µç·¨ç¢¼
        temporal_features = agent.tgn_encoder.forward(
            obs['edge_df'],    # é‚Šç‰¹å¾µ (max_edges, 7)
            obs['svc_df'],     # ç¯€é»ç‰¹å¾µ (max_nodes, 6)
            obs['edge_mask'],  # é‚Šé®ç½©
            obs['node_mask']   # ç¯€é»é®ç½©
        )
        
        # æ›´æ–°TGNè¨˜æ†¶é«”
        agent.tgn_encoder.update_memory(src, dst, timestamps, messages)
    
    # 2. GNNåœ–çµæ§‹ç·¨ç¢¼
    graph_features = agent.gnn_encoder.forward(
        obs['svc_df'],      # ç¯€é»ç‰¹å¾µ
        obs['edge_df'],     # é‚Šç‰¹å¾µ  
        obs['node_mask']    # ç¯€é»é®ç½©
    )
    
    # 3. ç‰¹å¾µèåˆ
    if agent.use_tgn:
        combined_features = temporal_features + graph_features
    else:
        combined_features = graph_features
    
    return combined_features
```

### Phase 4: ç­–ç•¥æ±ºç­–
```python
def policy_decision_flow(combined_features, obs):
    # 1. ç­–ç•¥ç¶²è·¯é æ¸¬
    action_logits, value = agent.policy_head(combined_features)
    
    # 2. æ‡‰ç”¨å‹•ä½œé®ç½©
    masked_logits = action_logits.masked_fill(
        obs['invalid_action_mask'].bool(), float('-inf')
    )
    
    # 3. æ¡æ¨£å‹•ä½œ
    action_dist = torch.distributions.Categorical(logits=masked_logits)
    action = action_dist.sample()
    
    # 4. åŸ·è¡Œå‹•ä½œ
    obs, reward, done, info = env.step(action)
    
    return action, reward, obs
```

### Phase 5: å‹•ä½œåŸ·è¡Œèˆ‡ç’°å¢ƒæ›´æ–°
```python
def action_execution_flow(action):
    # 1. è§£æå‹•ä½œ
    deployment_id = action[0]  # é¸æ“‡çš„æœå‹™
    move_id = action[1]        # é¸æ“‡çš„å‹•ä½œé¡å‹
    
    # 2. åŸ·è¡ŒKubernetesæ“ä½œ
    if move_id == ACTION_ADD_1_REPLICA:
        env.deploymentList[deployment_id].deploy_pod_replicas(1, env)
    elif move_id == ACTION_TERMINATE_1_REPLICA:
        env.deploymentList[deployment_id].terminate_pod_replicas(1, env)
    
    # 3. ç­‰å¾…Kubernetesæ›´æ–°
    if env.k8s and move_id != ACTION_DO_NOTHING:
        time.sleep(env.waiting_period)
    
    # 4. æ›´æ–°è§€å¯Ÿå€¼
    for d in env.deploymentList:
        d.update_obs_k8s()  # å¾Kubernetesç²å–æœ€æ–°ç‹€æ…‹
    
    # 5. è¨ˆç®—çå‹µ
    reward = env.get_reward  # åŸºæ–¼å»¶é²æˆ–æˆæœ¬çš„çå‹µ
```

### Phase 6: å­¸ç¿’æ›´æ–°
```python
def learning_update_flow():
    # 1. æ”¶é›†è»Œè·¡æ•¸æ“š
    trajectories = []
    for step in range(max_steps):
        action, reward, next_obs = policy_decision_flow(features, obs)
        trajectories.append((obs, action, reward, next_obs))
        obs = next_obs
    
    # 2. è¨ˆç®—å„ªå‹¢å‡½æ•¸
    advantages = compute_gae(trajectories, gamma=0.99, lambda_=0.95)
    
    # 3. PPOæ›´æ–°
    for epoch in range(ppo_epochs):
        # ç­–ç•¥æå¤±
        policy_loss = compute_policy_loss(trajectories, advantages)
        
        # åƒ¹å€¼æå¤±
        value_loss = compute_value_loss(trajectories)
        
        # ç¸½æå¤±
        total_loss = policy_loss + value_loss
        
        # åå‘å‚³æ’­
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
```

## ğŸ“Š é—œéµæ•¸æ“šæµ

### è§€å¯Ÿç©ºé–“çµæ§‹
```python
observation = {
    'svc_df': np.array(shape=(20, 6)),      # ç¯€é»ç‰¹å¾µ [pods, replicas, cpu, mem, traffic_in, traffic_out]
    'edge_df': np.array(shape=(400, 7)),    # é‚Šç‰¹å¾µ [src, dst, active, qps, p95, err_rate, mtls]
    'global_feats': np.array(shape=(4,)),   # å…¨å±€ç‰¹å¾µ [total_pods, avg_cpu, avg_mem, total_traffic]
    'node_mask': np.array(shape=(20,)),     # ç¯€é»æœ‰æ•ˆæ€§é®ç½©
    'edge_mask': np.array(shape=(400,)),    # é‚Šæœ‰æ•ˆæ€§é®ç½©
    'num_nodes': int,                       # ç•¶å‰æ´»èºç¯€é»æ•¸
    'invalid_action_mask': np.array(shape=(165,))  # ç„¡æ•ˆå‹•ä½œé®ç½©
}
```

### å‹•ä½œç©ºé–“çµæ§‹
```python
action_space = MultiDiscrete([15] * 11)  # 11å€‹æœå‹™ï¼Œæ¯å€‹æœå‹™15ç¨®å‹•ä½œ
# å‹•ä½œé¡å‹ï¼š
# 0: DO_NOTHING
# 1-7: ADD_1_REPLICA to ADD_7_REPLICA  
# 8-14: TERMINATE_1_REPLICA to TERMINATE_7_REPLICA
```

## ğŸ”§ ç³»çµ±é…ç½®

### å‹•æ…‹åœ–é…ç½®
```python
config = DynamicGraphConfig(
    max_nodes=20,           # æœ€å¤§ç¯€é»æ•¸
    max_edges=400,          # æœ€å¤§é‚Šæ•¸ (20*20)
    node_feat_dim=6,        # ç¯€é»ç‰¹å¾µç¶­åº¦
    edge_feat_dim=7,        # é‚Šç‰¹å¾µç¶­åº¦
    global_feat_dim=4       # å…¨å±€ç‰¹å¾µç¶­åº¦
)
```

### TGNé…ç½®
```python
tgn_config = {
    'max_nodes': 20,
    'in_dim': 6,
    'memory_dim': 32,
    'msg_dim': 32,
    'heads': 2
}
```

### PPOé…ç½®
```python
ppo_config = {
    'lr': 3e-4,
    'gamma': 0.99,
    'lambda_': 0.95,
    'clip_ratio': 0.2,
    'value_coef': 0.5,
    'entropy_coef': 0.01,
    'max_grad_norm': 0.5
}
```

## ğŸš€ é‹è¡Œç¯„ä¾‹

### å®Œæ•´è¨“ç·´æµç¨‹
```python
# 1. ç’°å¢ƒåˆå§‹åŒ–
env = OnlineBoutique(k8s=True, use_graph=True, goal_reward="latency")
agent = PPO_GNN(env.observation_space, env.action_space, use_tgn=True)

# 2. è¨“ç·´å¾ªç’°
for episode in range(num_episodes):
    obs = env.reset()
    total_reward = 0
    
    for step in range(max_steps):
        # ç²å–å‹•ä½œ
        action = agent.act(obs)
        
        # åŸ·è¡Œå‹•ä½œ
        next_obs, reward, done, info = env.step(action)
        
        # å­˜å„²ç¶“é©—
        agent.store_transition(obs, action, reward, next_obs, done)
        
        obs = next_obs
        total_reward += reward
        
        if done:
            break
    
    # æ›´æ–°ç­–ç•¥
    agent.update()
    
    print(f"Episode {episode}, Total Reward: {total_reward}")
```

## ğŸ¯ æ€§èƒ½å„ªåŒ–

### è¨˜æ†¶é«”ç®¡ç†
- TGNè¨˜æ†¶é«”ç‹€æ…‹çš„æœ‰æ•ˆé‡ç”¨
- å‹•æ…‹paddingé¿å…ä¸å¿…è¦çš„è¨ˆç®—
- é®ç½©æ©Ÿåˆ¶æ¸›å°‘ç„¡æ•ˆç‰¹å¾µè™•ç†

### è¨ˆç®—æ•ˆç‡
- æ‰¹é‡è™•ç†å¤šå€‹ç’°å¢ƒå¯¦ä¾‹
- GPUä¸¦è¡ŒåŒ–åœ–ç¥ç¶“ç¶²è·¯è¨ˆç®—
- ç•°æ­¥Kubernetes APIèª¿ç”¨

### æ”¶æ–‚ç©©å®šæ€§
- å‹•ä½œé®ç½©é¿å…ç„¡æ•ˆå‹•ä½œ
- çå‹µæ¨™æº–åŒ–æ”¹å–„è¨“ç·´ç©©å®šæ€§
- æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

## ğŸ“ˆ è©•ä¼°æŒ‡æ¨™

### ç³»çµ±æ€§èƒ½
- å¹³å‡å»¶é²é™ä½ç™¾åˆ†æ¯”
- è³‡æºåˆ©ç”¨ç‡æå‡
- æˆæœ¬ç¯€ç´„æ•ˆæœ

### å­¸ç¿’æ•ˆç‡
- æ”¶æ–‚é€Ÿåº¦ï¼ˆepisodes to convergenceï¼‰
- æ¨£æœ¬æ•ˆç‡ï¼ˆsample efficiencyï¼‰
- ç­–ç•¥ç©©å®šæ€§

### å‹•æ…‹é©æ‡‰æ€§
- ç¯€é»è®ŠåŒ–é©æ‡‰é€Ÿåº¦
- è¨˜æ†¶é«”é‡ç”¨æ•ˆæœ
- æ‹“æ’²è®ŠåŒ–é­¯æ£’æ€§

é€™å€‹å®Œæ•´çš„é‹ä½œéç¨‹å±•ç¤ºäº†GNNRLç³»çµ±å¦‚ä½•å¾æ•¸æ“šç²å–ã€åœ–ç¥ç¶“ç¶²è·¯è™•ç†ã€ç­–ç•¥æ±ºç­–åˆ°å‹•ä½œåŸ·è¡Œçš„å…¨æµç¨‹ï¼Œå¯¦ç¾äº†çœŸæ­£çš„å‹•æ…‹åœ–çµæ§‹æ”¯æ´å’Œé«˜æ•ˆçš„å¼·åŒ–å­¸ç¿’è¨“ç·´ã€‚