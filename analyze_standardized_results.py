#!/usr/bin/env python3
"""
æ¨™æº–åŒ–å¯¦é©—çµæœåˆ†æè…³æœ¬ (ç°¡åŒ–ç‰ˆ)
==========================================

é‡å°ä½¿ç”¨ç›¸åŒ8å€‹å ´æ™¯çš„ä¸‰æ–¹æ³•æ¯”è¼ƒå¯¦é©—ï¼Œæä¾›åŸºç¤æ€§èƒ½åˆ†æã€‚
ä¸ä¾è³´ pandas/numpy ç­‰å¤–éƒ¨åº«ã€‚
"""

import os
import json
import csv
from pathlib import Path
from typing import Dict, List
from datetime import datetime


class SimpleStandardizedAnalyzer:
    """ç°¡åŒ–ç‰ˆæ¨™æº–åŒ–çµæœåˆ†æå™¨"""
    
    def __init__(self, repo_root: Path = None):
        self.repo_root = repo_root or Path(__file__).parent
        self.logs_dir = self.repo_root / "logs"
        self.scenario_config = self._load_scenario_config()
        
    def _load_scenario_config(self) -> Dict:
        """è¼‰å…¥æ¨™æº–åŒ–å ´æ™¯é…ç½®"""
        config_file = self.repo_root / "standardized_test_scenarios.json"
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def find_standardized_experiments(self) -> Dict[str, List[Path]]:
        """æŸ¥æ‰¾æ¨™æº–åŒ–å¯¦é©—çµæœç›®éŒ„"""
        experiments = {
            'gym_hpa': [],
            'gnnrl': [],
            'k8s_hpa': []
        }
        
        # æŸ¥æ‰¾æ¨™æº–åŒ–å¯¦é©—ç›®éŒ„ - æ›´æ–°ç‚ºå¯¦éš›çš„ç›®éŒ„å‘½åæ¨¡å¼
        patterns = {
            'gym_hpa': "*_test_seed42_*",
            'gnnrl': "*_test_seed42_*", 
            'k8s_hpa': "*_cpu_seed42_*"
        }
        
        for method in experiments.keys():
            method_dir = self.logs_dir / method.replace('_', '-')
            if method_dir.exists():
                dirs = list(method_dir.glob(patterns[method]))
                experiments[method] = dirs
                print(f"æ‰¾åˆ° {method} æ¨™æº–åŒ–å¯¦é©—: {len(dirs)} å€‹")
                for dir_path in dirs:
                    print(f"  - {dir_path.name}")
            
        return experiments
    
    def analyze_single_scenario(self, scenario_dir: Path) -> Dict:
        """åˆ†æå–®å€‹å ´æ™¯çš„çµæœ"""
        # æŸ¥æ‰¾ CSV çµ±è¨ˆæ–‡ä»¶
        stats_files = list(scenario_dir.glob("*_stats.csv"))
        if not stats_files:
            return {}
            
        stats_file = stats_files[0]
        try:
            with open(stats_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                rows = list(reader)
                
            if not rows:
                return {}
                
            # æ‰¾åˆ° Aggregated è¡Œæˆ–ä½¿ç”¨æœ€å¾Œä¸€è¡Œ
            agg_row = None
            for row in rows:
                if row.get('Name') == 'Aggregated':
                    agg_row = row
                    break
            
            if agg_row is None:
                agg_row = rows[-1]
            
            return {
                'requests': int(agg_row.get('Request Count', 0)),
                'failures': int(agg_row.get('Failure Count', 0)),
                'failure_rate': float(agg_row.get('Failure Count', 0)) / max(int(agg_row.get('Request Count', 1)), 1) * 100,
                'avg_response_time': float(agg_row.get('Average Response Time', 0)),
                'median_response_time': float(agg_row.get('Median Response Time', 0)),
                'p95_response_time': float(agg_row.get('95%', 0)),
                'rps': float(agg_row.get('Requests/s', 0))
            }
            
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•åˆ†æå ´æ™¯ {scenario_dir.name}: {e}")
            return {}
    
    def analyze_experiment_results(self, exp_dir: Path) -> Dict:
        """åˆ†æå–®å€‹å¯¦é©—ç›®éŒ„çš„æ‰€æœ‰å ´æ™¯"""
        scenario_results = {}
        
        # é æœŸçš„8å€‹æ¨™æº–å ´æ™¯
        expected_scenarios = [s['id'] for s in self.scenario_config.get('scenarios', [])]
        
        for scenario_id in expected_scenarios:
            scenario_dir = exp_dir / scenario_id
            if scenario_dir.exists():
                result = self.analyze_single_scenario(scenario_dir)
                if result:
                    result['scenario_type'] = self._get_scenario_type(scenario_id)
                    scenario_results[scenario_id] = result
                    
        return scenario_results
    
    def _get_scenario_type(self, scenario_id: str) -> str:
        """å¾å ´æ™¯IDç²å–é¡å‹"""
        for scenario in self.scenario_config.get('scenarios', []):
            if scenario['id'] == scenario_id:
                return scenario['type']
        return scenario_id.split('_')[0]  # fallback
    
    def calculate_method_summary(self, scenario_results: Dict) -> Dict:
        """è¨ˆç®—æ–¹æ³•çš„ç¸½é«”æŒ‡æ¨™"""
        if not scenario_results:
            return {}
            
        total_requests = sum(r['requests'] for r in scenario_results.values())
        total_failures = sum(r['failures'] for r in scenario_results.values())
        
        # åŠ æ¬Šå¹³å‡éŸ¿æ‡‰æ™‚é–“
        weighted_avg_response = 0
        if total_requests > 0:
            weighted_avg_response = sum(
                r['avg_response_time'] * r['requests'] for r in scenario_results.values()
            ) / total_requests
        
        # å¹³å‡æŒ‡æ¨™
        avg_failure_rate = sum(r['failure_rate'] for r in scenario_results.values()) / len(scenario_results)
        avg_p95 = sum(r['p95_response_time'] for r in scenario_results.values()) / len(scenario_results)
        avg_rps = sum(r['rps'] for r in scenario_results.values()) / len(scenario_results)
        
        return {
            'scenarios_tested': len(scenario_results),
            'total_requests': total_requests,
            'total_failures': total_failures,
            'overall_failure_rate': (total_failures / max(total_requests, 1)) * 100,
            'avg_failure_rate': avg_failure_rate,
            'weighted_avg_response_time': weighted_avg_response,
            'avg_p95': avg_p95,
            'avg_rps': avg_rps
        }
    
    def generate_comparison_report(self):
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        print("ğŸ” é–‹å§‹æ¨™æº–åŒ–å¯¦é©—çµæœåˆ†æ...")
        
        # æŸ¥æ‰¾å¯¦é©—
        experiments = self.find_standardized_experiments()
        
        if not any(experiments.values()):
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨™æº–åŒ–å¯¦é©—çµæœ")
            return
        
        # åˆ†ææ¯å€‹æ–¹æ³•
        method_summaries = {}
        scenario_details = {}
        
        for method, exp_dirs in experiments.items():
            if not exp_dirs:
                continue
                
            # å–æœ€æ–°çš„å¯¦é©—ç›®éŒ„
            latest_exp = max(exp_dirs, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“Š åˆ†æ {method} å¯¦é©—: {latest_exp.name}")
            
            scenario_results = self.analyze_experiment_results(latest_exp)
            method_summaries[method] = self.calculate_method_summary(scenario_results)
            scenario_details[method] = scenario_results
        
        # ç”Ÿæˆå ±å‘Š
        self._print_comparison_summary(method_summaries)
        self._print_scenario_details(scenario_details)
        self._save_csv_reports(method_summaries, scenario_details)
        self._generate_markdown_report(method_summaries, scenario_details)
        
        print("âœ… æ¨™æº–åŒ–å¯¦é©—åˆ†æå®Œæˆï¼")
    
    def _print_comparison_summary(self, summaries: Dict):
        """æ‰“å°æ¯”è¼ƒæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨™æº–åŒ–æ–¹æ³•æ¯”è¼ƒæ‘˜è¦")
        print("="*60)
        
        # è¡¨é ­
        print(f"{'Method':<12} {'Scenarios':<9} {'Requests':<10} {'Avg RT(ms)':<11} {'Failure%':<9} {'P95(ms)':<9} {'RPS':<7}")
        print("-" * 70)
        
        # æ•¸æ“šè¡Œ
        for method, summary in summaries.items():
            method_name = method.upper().replace('_', '-')
            print(f"{method_name:<12} "
                  f"{summary.get('scenarios_tested', 0):<9} "
                  f"{summary.get('total_requests', 0):<10,} "
                  f"{summary.get('weighted_avg_response_time', 0):<11.1f} "
                  f"{summary.get('avg_failure_rate', 0):<9.2f} "
                  f"{summary.get('avg_p95', 0):<9.0f} "
                  f"{summary.get('avg_rps', 0):<7.1f}")
    
    def _print_scenario_details(self, details: Dict):
        """æ‰“å°å ´æ™¯è©³ç´°ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“‹ å ´æ™¯ç´šåˆ¥è©³ç´°æ¯”è¼ƒ")
        print("="*60)
        
        # ç²å–æ‰€æœ‰å ´æ™¯
        all_scenarios = set()
        for method_scenarios in details.values():
            all_scenarios.update(method_scenarios.keys())
        
        for scenario_id in sorted(all_scenarios):
            scenario_type = self._get_scenario_type(scenario_id)
            print(f"\nğŸ¯ {scenario_id} ({scenario_type})")
            print(f"{'Method':<12} {'Requests':<10} {'RT(ms)':<8} {'Failure%':<9} {'P95(ms)':<9} {'RPS':<7}")
            print("-" * 56)
            
            for method, scenarios in details.items():
                if scenario_id in scenarios:
                    result = scenarios[scenario_id]
                    method_name = method.upper().replace('_', '-')
                    print(f"{method_name:<12} "
                          f"{result['requests']:<10,} "
                          f"{result['avg_response_time']:<8.1f} "
                          f"{result['failure_rate']:<9.2f} "
                          f"{result['p95_response_time']:<9.0f} "
                          f"{result['rps']:<7.1f}")
                else:
                    method_name = method.upper().replace('_', '-')
                    print(f"{method_name:<12} {'N/A':<10} {'N/A':<8} {'N/A':<9} {'N/A':<9} {'N/A':<7}")
    
    def _save_csv_reports(self, summaries: Dict, details: Dict):
        """ä¿å­˜ CSV å ±å‘Š"""
        output_dir = self.logs_dir
        
        # æ–¹æ³•æ¯”è¼ƒ CSV
        method_file = output_dir / "standardized_method_comparison.csv"
        with open(method_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Method', 'Scenarios', 'Total Requests', 'Avg Response Time (ms)', 
                           'Avg Failure Rate (%)', 'Avg P95 (ms)', 'Avg RPS'])
            
            for method, summary in summaries.items():
                writer.writerow([
                    method.upper().replace('_', '-'),
                    summary.get('scenarios_tested', 0),
                    summary.get('total_requests', 0),
                    round(summary.get('weighted_avg_response_time', 0), 2),
                    round(summary.get('avg_failure_rate', 0), 2),
                    round(summary.get('avg_p95', 0), 2),
                    round(summary.get('avg_rps', 0), 2)
                ])
        
        print(f"âœ… æ–¹æ³•æ¯”è¼ƒçµæœ: {method_file}")
        
        # å ´æ™¯æ¯”è¼ƒ CSV
        scenario_file = output_dir / "standardized_scenario_comparison.csv"
        with open(scenario_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Scenario ID', 'Scenario Type', 'Method', 'Requests', 
                           'Failure Rate (%)', 'Avg Response Time (ms)', 'P95 Response Time (ms)', 'RPS'])
            
            all_scenarios = set()
            for method_scenarios in details.values():
                all_scenarios.update(method_scenarios.keys())
            
            for scenario_id in sorted(all_scenarios):
                scenario_type = self._get_scenario_type(scenario_id)
                for method, scenarios in details.items():
                    if scenario_id in scenarios:
                        result = scenarios[scenario_id]
                        writer.writerow([
                            scenario_id,
                            scenario_type,
                            method.upper().replace('_', '-'),
                            result['requests'],
                            round(result['failure_rate'], 2),
                            round(result['avg_response_time'], 2),
                            round(result['p95_response_time'], 2),
                            round(result['rps'], 2)
                        ])
        
        print(f"âœ… å ´æ™¯æ¯”è¼ƒçµæœ: {scenario_file}")
    
    def _generate_markdown_report(self, summaries: Dict, details: Dict):
        """ç”Ÿæˆ Markdown å ±å‘Š"""
        report_content = f"""# æ¨™æº–åŒ–ä¸‰æ–¹æ³•è‡ªå‹•ç¸®æ”¾æ¯”è¼ƒå ±å‘Š

## ğŸ“‹ å¯¦é©—æ¦‚è¿°

**å¯¦é©—æ™‚é–“**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}
**æ¯”è¼ƒæ–¹æ³•**: Gym-HPA, GNNRL, K8s-HPA
**æ¸¬è©¦å ´æ™¯**: 8å€‹æ¨™æº–åŒ–å ´æ™¯ (åŸºæ–¼å›ºå®šç¨®å­ç”Ÿæˆ)

## ğŸ¯ æ¨™æº–åŒ–æ¸¬è©¦å ´æ™¯

"""
        
        # æ·»åŠ å ´æ™¯é…ç½®
        if self.scenario_config.get('scenarios'):
            report_content += "| åºè™Ÿ | å ´æ™¯ID | é¡å‹ | æè¿° |\n"
            report_content += "|-----|--------|------|------|\n"
            for i, scenario in enumerate(self.scenario_config['scenarios'], 1):
                report_content += f"| {i} | {scenario['id']} | {scenario['type']} | {scenario['description']} |\n"
        
        # æ·»åŠ æ–¹æ³•æ¯”è¼ƒ
        if summaries:
            report_content += "\n## ğŸ“Š æ–¹æ³•ç¸½é«”æ€§èƒ½æ¯”è¼ƒ\n\n"
            report_content += "| æ–¹æ³• | å ´æ™¯æ•¸ | ç¸½è«‹æ±‚æ•¸ | å¹³å‡éŸ¿æ‡‰æ™‚é–“(ms) | å¹³å‡å¤±æ•—ç‡(%) | å¹³å‡P95(ms) | å¹³å‡RPS |\n"
            report_content += "|-----|--------|----------|-----------------|--------------|-------------|--------|\n"
            
            for method, summary in summaries.items():
                method_name = method.upper().replace('_', '-')
                report_content += f"| {method_name} | {summary.get('scenarios_tested', 0)} | {summary.get('total_requests', 0):,} | {summary.get('weighted_avg_response_time', 0):.1f} | {summary.get('avg_failure_rate', 0):.2f} | {summary.get('avg_p95', 0):.0f} | {summary.get('avg_rps', 0):.1f} |\n"
        
        # æ·»åŠ çµè«–
        report_content += """

## ğŸ’¡ é—œéµç™¼ç¾

### 1. æ•´é«”æ€§èƒ½
åŸºæ–¼æ¨™æº–åŒ–çš„8å€‹å ´æ™¯æ¸¬è©¦ï¼Œå„æ–¹æ³•åœ¨ä¸åŒè² è¼‰æ¨¡å¼ä¸‹è¡¨ç¾å‡ºä¸åŒç‰¹æ€§ã€‚

### 2. å»ºè­°

**ç”Ÿç”¢ç’°å¢ƒé¸æ“‡**:
- éœ€è¦è©³ç´°åˆ†æä¸Šè¿°æ•¸æ“šä¾†ç¢ºå®šæœ€é©åˆçš„æ–¹æ³•

---
*å ±å‘Šç”±æ¨™æº–åŒ–å¯¦é©—åˆ†æå™¨è‡ªå‹•ç”Ÿæˆ*
"""
        
        # ä¿å­˜å ±å‘Š
        report_file = self.repo_root / "STANDARDIZED_COMPARISON_REPORT.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… æ¨™æº–åŒ–æ¯”è¼ƒå ±å‘Š: {report_file}")


def main():
    """ä¸»å‡½æ•¸"""
    analyzer = SimpleStandardizedAnalyzer()
    analyzer.generate_comparison_report()


if __name__ == "__main__":
    main()